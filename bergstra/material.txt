
Theano's
expression graphs can contain information about the shape, number of
dimensions as well as "broadcastability" of most variables, so Theano
can specialize many operations.
The end user is offered an interface similar to NumPy's to 
declare variables and expressions involving those variables, creating
a symbolic representation of the computations to perform. Any number of expressions
can then be compiled into a highly optimized function taking ``numpy.ndarray`` instances as
arguments and yielding the result(s) of the expression(s).

The general principle of Theano is to describe calculations as a high-level mathematical graph.
This symbolic description
is separate from execution. Before executing the mathematical graph, the user must once compile it into a callable function.
This callable function can then be executed repeatedly on a variety of inputs.
There are several advantages to this approach:
Theano lets user code stay clean, easy to read and familiar to NumPy users,
while the ugly performance-oriented hacks are inserted automatically at
compile time.
For example, large expressions involving addition, subtraction, multiplication, and matrix products are rearranged to make the best use of low-level BLAS subroutines [BLAS]_.
Besides this user-facing advantage, there are backend advantages to separating the description of the calculations from their execution.


Using Theano
------------
There are four conceptual steps to using Theano: 1) declaring variables,
2) using these variables in expressions, 3) compiling these expressions into functions,
and 4) calling these functions to perform the numerical computations of interest.
These four steps are illustrated in the following simplistic example.

.. _Listing 1:
.. _FigureSimple:

.. figure:: fig_simple.pdf
    :scale: 100

    **Listing 1: A simple Theano program.**

Line 2 declares a Theano variable. Unlike Python's variables, Theano's variables and expressions are
statically typed. For example, tensor type information includes
the data type (``int32``, ``float32``, ``float64``, etc.), the number of dimensions (scalar=0, vector=1, etc.),
and - for each dimension - whether it is broadcastable or not.
Broadcasting in
Theano is similar to that in NumPy, except that broadcasting decisions are made
based on the type information rather than the shape of the actual ndarrays.
A variable declared as ``theano.tensor.vector`` has one non-broadcastable
dimension.
In the example, ``a`` stands for a 1-dimensional ndarray of any number of elements
of the default data type (``float64``), and
even if that length is just one element, ``a`` will not be broadcasted to behave like a longer
vector.
Variable names are optional, but we have given our variable the name ``'a'``.
Theano variable names are purely decorative, they are used only by printing and visualization routines.


.. _Figure 1:
.. _FigureSimpleUnopt:

.. figure:: f_unoptimized.pdf
    :scale: 100

    **Figure 1:**
    Unoptimized expression graph for function ``f`` (`Listing 1`_). 
    This image is generated by:

    ::

        theano.printing.pydotprint(
            theano.function([a], b, mode="FAST_COMPILE"))


.. _Figure 2:
.. _FigureSimpleOpt:

.. figure:: f_optimized.pdf
    :scale: 100

    **Figure 2:**
    Optimized expression graph from `Figure 1`_, 
    in which :math:`$a+a^{10}$` was replaced by :math:`$\left(a+a^{2^{2^2}}a^2\right)$` to 
    avoid a call to ``pow``.
    This image is generated by: ``theano.printing.pydotprint(f)``

Line 3 builds the expression graph shown in `Figure 1`_.
The green rectangles in `Figure 1`_ are the inputs: on the left is the ``a``
variable, and on the right is a constant (``val=[10]`` in the label) that Theano has interpreted as
a tensor (``TensorType``) of data type 'int8' that has one broadcastable
dimension (``(True,)``).
The integers on the arcs indicate the argument position (in the destination).

Line 4 transforms the expression graph to the one shown in `Figure 2`_,
compiles C++ code for the remaining expressions, and returns the result as a 
callable function ``f``.
The ``add`` and ``pow`` expressions are transformed into a single compound
expression labelled ``Elemwise{Composite{...}}`` that computes our expression
with a single loop over ``a`` and no calls to ``pow``.


Notice that we used an optional ``mode`` argument to ``function`` to produce 
`Figure 1`_.
The ``mode`` argument controls which graph
transformations to use, whether to use C, and how Theano will actually execute the computations.
`Figure 1`_ was produced with the
``'FAST_COMPILE'`` mode performs minimal graph transformations and ignores C++
implementations.
`Figure 2`_ was produced with the default
``'FAST_RUN'`` mode which attempts many graph transformations and uses C++ implementations.
There are other modes too: ``'PROFILE_MODE'`` measures where Theano's compiled
functions spend their time and ``'DEBUG_MODE'`` runs many redundant calculations
and verifications to detect potential errors introduced by graph
transformations.
Theano is verified nightly by thousands of unit tests, but it is still advised
to run your functions in ``'DEBUG_MODE'`` periodically (on small data because it is slow!) 
to be sure that your results are correct.


The last line of the example (``print f([0,1,2])``) calls our function ``f`` and
computes our expression with the numerical values ``[0,1,2]``  provided as vector ``a``.
The types of the arguments and the return value of ``f`` are defined
by the input variables and the output variables we provided to ``theano.function`` on Line 3.
In our example, all computations are done on ``numpy.ndarray`` instances, one of
which is returned to the user. 
Type conversions are performed automatically as necessary - the Python list
of Python integers given in the example is internally converted to a ``numpy.ndarray``
of doubles, to match the declaration of variable ``a``.





This section introduces basic usage of Theano, as well as
This section describes a more involved example to illustrate some useful Theano features:
*shared variables*, which make code more concise and make it easy to take advantage of an available GPU;
automatic differentiation; and some graph transformations that bring numerical stability
and speed improvements, even on a CPU.
