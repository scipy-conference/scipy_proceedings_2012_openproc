
Theano's
expression graphs can contain information about the shape, number of
dimensions as well as "broadcastability" of most variables, so Theano
can specialize many operations.
The end user is offered an interface similar to NumPy's to 
declare variables and expressions involving those variables, creating
a symbolic representation of the computations to perform. Any number of expressions
can then be compiled into a highly optimized function taking ``numpy.ndarray`` instances as
arguments and yielding the result(s) of the expression(s).

The general principle of Theano is to describe calculations as a high-level mathematical graph.
This symbolic description
is separate from execution. Before executing the mathematical graph, the user must once compile it into a callable function.
This callable function can then be executed repeatedly on a variety of inputs.
There are several advantages to this approach:
Theano lets user code stay clean, easy to read and familiar to NumPy users,
while the ugly performance-oriented hacks are inserted automatically at
compile time.
For example, large expressions involving addition, subtraction, multiplication, and matrix products are rearranged to make the best use of low-level BLAS subroutines [BLAS]_.
Besides this user-facing advantage, there are backend advantages to separating the description of the calculations from their execution.


Using Theano
------------
There are four conceptual steps to using Theano: 1) declaring variables,
2) using these variables in expressions, 3) compiling these expressions into functions,
and 4) calling these functions to perform the numerical computations of interest.
These four steps are illustrated in the following simplistic example.

.. _Listing 1:
.. _FigureSimple:

.. figure:: fig_simple.pdf
    :scale: 100

    **Listing 1: A simple Theano program.**

Line 2 declares a Theano variable. Unlike Python's variables, Theano's variables and expressions are
statically typed. For example, tensor type information includes
the data type (``int32``, ``float32``, ``float64``, etc.), the number of dimensions (scalar=0, vector=1, etc.),
and - for each dimension - whether it is broadcastable or not.
Broadcasting in
Theano is similar to that in NumPy, except that broadcasting decisions are made
based on the type information rather than the shape of the actual ndarrays.
A variable declared as ``theano.tensor.vector`` has one non-broadcastable
dimension.
In the example, ``a`` stands for a 1-dimensional ndarray of any number of elements
of the default data type (``float64``), and
even if that length is just one element, ``a`` will not be broadcasted to behave like a longer
vector.
Variable names are optional, but we have given our variable the name ``'a'``.
Theano variable names are purely decorative, they are used only by printing and visualization routines.


.. _Figure 1:
.. _FigureSimpleUnopt:

.. figure:: f_unoptimized.pdf
    :scale: 100

    **Figure 1:**
    Unoptimized expression graph for function ``f`` (`Listing 1`_). 
    This image is generated by:

    ::

        theano.printing.pydotprint(
            theano.function([a], b, mode="FAST_COMPILE"))


.. _Figure 2:
.. _FigureSimpleOpt:

.. figure:: f_optimized.pdf
    :scale: 100

    **Figure 2:**
    Optimized expression graph from `Figure 1`_, 
    in which :math:`$a+a^{10}$` was replaced by :math:`$\left(a+a^{2^{2^2}}a^2\right)$` to 
    avoid a call to ``pow``.
    This image is generated by: ``theano.printing.pydotprint(f)``

Line 3 builds the expression graph shown in `Figure 1`_.
The green rectangles in `Figure 1`_ are the inputs: on the left is the ``a``
variable, and on the right is a constant (``val=[10]`` in the label) that Theano has interpreted as
a tensor (``TensorType``) of data type 'int8' that has one broadcastable
dimension (``(True,)``).
The integers on the arcs indicate the argument position (in the destination).

Line 4 transforms the expression graph to the one shown in `Figure 2`_,
compiles C++ code for the remaining expressions, and returns the result as a 
callable function ``f``.
The ``add`` and ``pow`` expressions are transformed into a single compound
expression labelled ``Elemwise{Composite{...}}`` that computes our expression
with a single loop over ``a`` and no calls to ``pow``.


Notice that we used an optional ``mode`` argument to ``function`` to produce 
`Figure 1`_.
The ``mode`` argument controls which graph
transformations to use, whether to use C, and how Theano will actually execute the computations.
`Figure 1`_ was produced with the
``'FAST_COMPILE'`` mode performs minimal graph transformations and ignores C++
implementations.
`Figure 2`_ was produced with the default
``'FAST_RUN'`` mode which attempts many graph transformations and uses C++ implementations.
There are other modes too: ``'PROFILE_MODE'`` measures where Theano's compiled
functions spend their time and ``'DEBUG_MODE'`` runs many redundant calculations
and verifications to detect potential errors introduced by graph
transformations.
Theano is verified nightly by thousands of unit tests, but it is still advised
to run your functions in ``'DEBUG_MODE'`` periodically (on small data because it is slow!) 
to be sure that your results are correct.


The last line of the example (``print f([0,1,2])``) calls our function ``f`` and
computes our expression with the numerical values ``[0,1,2]``  provided as vector ``a``.
The types of the arguments and the return value of ``f`` are defined
by the input variables and the output variables we provided to ``theano.function`` on Line 3.
In our example, all computations are done on ``numpy.ndarray`` instances, one of
which is returned to the user. 
Type conversions are performed automatically as necessary - the Python list
of Python integers given in the example is internally converted to a ``numpy.ndarray``
of doubles, to match the declaration of variable ``a``.


This section introduces basic usage of Theano, as well as
This section describes a more involved example to illustrate some useful Theano features:
*shared variables*, which make code more concise and make it easy to take advantage of an available GPU;
automatic differentiation; and some graph transformations that bring numerical stability
and speed improvements, even on a CPU.


Cut from old logistic regression section
----------------------------------------

* (Guillaume says) I believe this would be better suited to the "What's in
Theano - GPU" section.

Theano manages the storage of these values. In particular, it stores
single-precision dense *shared* tensors on the GPU by default when a GPU is
available.  In such cases it uses a different Theano-specific data type for
internal storage in place of the NumPy ``ndarray``.

On a GPU, this means that a shared variable and its updated value can all reside
on the device. Having both on the device can be
important for performance, because it is slow to copy between the host and the GPU.

* (Guillaume) This should go in the optimization section of "What's in Theano".
We should try as much as possible to have the Theano concepts refer back to
the logistic regression example.

Theano applies some graph transformations to optimize the ``train`` and
``predict`` functions for speed and numerical stability, when compiling them
in Lines 22-25 and 26, respectively.  For example, in the ``predict``
function, ``1/(1+exp(-u))`` is recognized as the logistic sigmoid function
and replaced with an implementation that is faster for large positive and
negative values of ``u``.  All the element-wise operations are fused
together after the vector-matrix multiplication and compiled as a
specialized C function with a single loop over the data.  In the ``train``
function, Theano additionally recognizes ``log(sigmoid(u))`` and
``log(1-sigmoid(u))`` as instances of the softplus function:
``log1p(exp(u))``, for which Theano has an implementation that avoids a
dangerous potential overflow.  When updating ``w`` with its new value,
Theano also recognizes that a single call to the BLAS ``dgemv`` routine can
implement the :math:`$\ell_2$`-regularization of ``w``, scale its gradient,
and decrement ``w`` by its scaled gradient.


SECTIONS REMOVED BY RAZVAN:
---------------------------

This section gives an overview the design of Theano.

A Theano expression graph is a bi-partite directed acyclic graph.
It is bi-partite because there are two kinds of nodes: *variable* nodes are the
inputs to and outputs from *apply* nodes.
A *variable* node represents input or an intermediate mathematical result.
It has a *Type* (``.type``) that signals the sort of value the variable might take at
runtime.
An *apply* node represents the application of the *Op* (``.op``) to some input *variables* (``.inputs``) producing some output *variables* (``.outputs``).
Figures 1 and 2 have been simplified for clarity.
Technically there is an
intermediate result for the output of the ``Elemwise{pow,no_inplace}``,
and the variable nodes (box) and apply nodes (ellipse) are distinct from the
Type and Op instances respectively (not shown) that give them meaning.


Variables
~~~~~~~~~~~~~~~~~~~

Theano supports three kinds of variable nodes: *Variables*, *Constants*, and *Shared variables*. 
*Variable* nodes (with a capital V) are the most common kind - a Variable is either found as a
leaf of the graph (if it was created explicitly with a call like ``theano.tensor.vector()``),
or as the output of an *apply* node (if it was defined by the application
of an Op).
In the latter case, the Variable will have a ``.owner`` attribute pointing to the *apply* node.
``a`` and ``b`` in `Listing 1`_ are Variables (without ``.owner``).
``p_1`` in `Listing 2`_ is also a Variable (with ``.owner``).
``theano.function`` takes two arguments: the input list, which is a list of Variables; and the output value or list, which is a Variable or list of Variables.
*Constant* nodes each have a ``.value`` attribute, which is the immutable (read-only) value of this variable.
``10`` in `Listing 1`_ was converted to a Constant node.
*Shared Variable* nodes have ``.get_value()`` and ``.set_value(new_val)`` methods that
behave by default as if they are transfering from and to (respectively) Theano-managed
memory. Sometimes this is done for consistency, and other times (like when a
type conversion takes place, or the transfer requires moving data to or from a
GPU) it is a necessary copy.
This value can also be modified by calling a Theano function that was defined with ``updates``, like ``train`` in `Listing 2`_.

Types
~~~~~~~~~~~~~~~~~~~

The important variable Types in Theano are:

 * ``TensorType`` - 
   denotes a ``numpy.ndarray`` with specific number of dimensions,
   a record of which of these dimensions are broadcastable, and *dtype*. The dtype is the data types,
   e.g. ``int32``, ``float64``, etc.

 * ``SparseType`` -
   denotes one of the ``csr`` or ``csc`` formats in ``scipy.sparse``.

 * ``RandomStateType`` -
   denotes a NumPy ``RandomState`` object. They are rarely used directly
   by Theano user code. They are storage containers for the random
   number generator.

 * ``Generic`` -
   denotes any Python value.
   They are rarely used directly by Theano user code.
   Generic Variables exist mainly for Ops to be able
   to allocate workspace outputs.


Theano types are often stricter
than their NumPy/SciPy equivalents. For example,
there are different versions of ``SparseType`` in Theano, which are specific
to different encodings like ``csr`` or ``csc``. The Theano ``TensorType`` that 
corresponds to a ``numpy.ndarray`` also specifies
the number of dimensions (scalar=0, vector=1, etc.), which of them are
broadcastable, and what *dtype* should be used. This information is used 
when performing graph transformations.

For *Shared Variables* and *Constants*, the type is inferred 
automatically based on the value given during initialization.
